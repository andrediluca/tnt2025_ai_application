{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "irish-silence",
      "metadata": {
        "id": "irish-silence"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "from matplotlib.colors import LogNorm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "634b83e3",
      "metadata": {},
      "source": [
        "Each file is a collection of 500,000 (or 400,000 in the case of positrons) calorimeter showers corresponding to the particle specified in the file name (eplus = positrons, gamma = photons, piplus = charged pions). \n",
        "\n",
        "The calorimeter we built is segmented longitudinally into three layer with different depths and granularities. In units of mm, the three layers have the following (eta, phi, z) dimensions:\n",
        "Layer 0: (5, 160, 90) | Layer 1: (40, 40, 347) | Layer 2: (80, 40, 43)\n",
        "\n",
        "In the HDF5 files, the `energy` entry specifies the true energy of the incoming particle in units of GeV. `layer_0`, `layer_1`, and `layer_2` represents the energy deposited in each layer of the calorimeter in an image data format. Given the segmentation of each calorimeter layer, these images have dimensions 3x96 (in layer 0), 12x12 (in layer 1), and 12x6 (in layer 3). The `overflow` contains the amount of energy that was deposited outside of the calorimeter section we are considering. The fields `x0`, `y0`, `z0`, `t0`, `px`, `py`, `pz` provide position and kinematic information of incident particles upon impact. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "australian-onion",
      "metadata": {
        "id": "australian-onion"
      },
      "outputs": [],
      "source": [
        "path = \"./data\"\n",
        "filename_e = 'eplus_angle_position_5deg_xy.h5'\n",
        "# filename_gamma = 'gamma_angle_position_5deg_xy.h5'\n",
        "# filename_pi = 'piplus_angle_position_5deg_xy.h5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "communist-adolescent",
      "metadata": {
        "id": "communist-adolescent",
        "outputId": "5afb29d5-6290-4ed6-a893-69ddb9ef2b4b"
      },
      "outputs": [],
      "source": [
        "data_e = h5py.File(os.path.join(path,filename_e),'r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15d5cfff",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_e.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30ea4b31",
      "metadata": {},
      "outputs": [],
      "source": [
        "#create a class to hold the data \n",
        "class ParticleData:\n",
        "    def __init__(self, data):\n",
        "        self.energy = np.array(data['energy'])\n",
        "        self.energy = np.array(data['energy'])\n",
        "        self.layer_0 = np.array(data['layer_0'])\n",
        "        self.layer_1 = np.array(data['layer_1'])\n",
        "        self.layer_2 = np.array(data['layer_2'])\n",
        "        self.px,self.py,self.pz = np.array(data['px']),np.array(data['py']),np.array(data['pz'])\n",
        "        self.x0,self.y0,self.z0 = np.array(data['x0']),np.array(data['y0']),np.array(data['z0'])\n",
        "    \n",
        "    def check_for_nulls_and_infs(self):\n",
        "        issues = {}\n",
        "        for attr_name in vars(self):\n",
        "            attr = getattr(self, attr_name)\n",
        "            if isinstance(attr, np.ndarray):\n",
        "                nan_count = np.isnan(attr).sum()\n",
        "                inf_count = np.isinf(attr).sum()\n",
        "                if nan_count > 0 or inf_count > 0:\n",
        "                    issues[attr_name] = {\n",
        "                        'nan_count': int(nan_count),\n",
        "                        'inf_count': int(inf_count)\n",
        "                    }\n",
        "            elif attr is None:\n",
        "                issues[attr_name] = {'none': True}\n",
        "        return issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c0cbdb",
      "metadata": {},
      "outputs": [],
      "source": [
        "particle_data = ParticleData(data_e)\n",
        "# data_gamma = ParticleData(data_gamma)\n",
        "# data_pi = ParticleData(data_pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4900d5c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "particle_data.check_for_nulls_and_infs()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6109e6c7",
      "metadata": {},
      "source": [
        "## Data Visuzalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a473ac18",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create subplots with 1 row and 3 columns\n",
        "fig, ax = plt.subplots(1, 3, figsize=(16, 7))\n",
        "\n",
        "# Plot histograms\n",
        "ax[0].hist(particle_data.x0, bins=100, color='skyblue', edgecolor='k')\n",
        "ax[0].set_title('x0 Distribution')\n",
        "ax[0].set_xlabel('x0')\n",
        "ax[0].set_ylabel('Count')\n",
        "\n",
        "ax[1].hist(particle_data.y0, bins=100, color='lightgreen', edgecolor='k')\n",
        "ax[1].set_title('y0 Distribution')\n",
        "ax[1].set_xlabel('y0')\n",
        "ax[1].set_ylabel('Count')\n",
        "\n",
        "ax[2].hist(particle_data.z0, bins=100, color='salmon', edgecolor='k')\n",
        "ax[2].set_title('z0 Distribution')\n",
        "ax[2].set_xlabel('z0')\n",
        "ax[2].set_ylabel('Count')\n",
        "\n",
        "# Layout adjustment\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "424d9eb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.hist(particle_data.energy, bins = np.linspace(0,100,70),  color='skyblue', edgecolor='k' )\n",
        "plt.xlabel('Particle Energy [GeV]')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "445c1db6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(6, 12))  # 3 rows, 1 column\n",
        "\n",
        "# Plot each layer for a single particle (index 0)\n",
        "axes[0].imshow(particle_data.layer_0[0], interpolation='nearest', norm=LogNorm(), origin='lower')\n",
        "axes[0].set_title('Particle 1 - Layer 0')\n",
        "plt.colorbar(mappable=axes[0].images[0], ax=axes[0])\n",
        "\n",
        "axes[1].imshow(particle_data.layer_1[0], interpolation='nearest', norm=LogNorm(), origin='lower')\n",
        "axes[1].set_title('Particle 1 - Layer 1')\n",
        "plt.colorbar(mappable=axes[1].images[0], ax=axes[1])\n",
        "\n",
        "axes[2].imshow(particle_data.layer_2[0], interpolation='nearest', norm=LogNorm(), origin='lower')\n",
        "axes[2].set_title('Particle 1 - Layer 2')\n",
        "plt.colorbar(mappable=axes[2].images[0], ax=axes[2])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "662bf8bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Split indices\n",
        "indices = np.arange(len(particle_data.energy))\n",
        "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compute statistics\n",
        "layer_0_flat = particle_data.layer_0[train_idx].flatten()\n",
        "layer_1_flat = particle_data.layer_1[train_idx].flatten()\n",
        "layer_2_flat = particle_data.layer_2[train_idx].flatten()\n",
        "energy = particle_data.energy[train_idx]\n",
        "\n",
        "stats = {\n",
        "    'l0_mean': np.mean(layer_0_flat),\n",
        "    'l0_std':  np.std(layer_0_flat),\n",
        "    'l1_mean': np.mean(layer_1_flat),\n",
        "    'l1_std':  np.std(layer_1_flat),\n",
        "    'l2_mean': np.mean(layer_2_flat),\n",
        "    'l2_std':  np.std(layer_2_flat),\n",
        "    'energy_mean': np.mean(energy),\n",
        "    'energy_std':  np.std(energy),\n",
        "}\n",
        "\n",
        "# Print stats\n",
        "print(\"Statistics for training set:\")\n",
        "for key, value in stats.items():\n",
        "    print(f\"{key}: {value:.2f}\")\n",
        "\n",
        "# Create raw and scaled feature dictionaries\n",
        "raw_features = {\n",
        "    'Layer 0': layer_0_flat,\n",
        "    'Layer 1': layer_1_flat,\n",
        "    'Layer 2': layer_2_flat,\n",
        "    'Energy': energy\n",
        "}\n",
        "\n",
        "scaled_features = {\n",
        "    'Layer 0': (layer_0_flat - stats['l0_mean']) / stats['l0_std'],\n",
        "    'Layer 1': (layer_1_flat - stats['l1_mean']) / stats['l1_std'],\n",
        "    'Layer 2': (layer_2_flat - stats['l2_mean']) / stats['l2_std'],\n",
        "    'Energy': (energy - stats['energy_mean']) / stats['energy_std']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b1ef3d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot original and scaled histograms\n",
        "plt.figure(figsize=(14, 10))\n",
        "for i, key in enumerate(raw_features.keys()):\n",
        "    # Raw plot\n",
        "    plt.subplot(4, 2, i*2 + 1)\n",
        "    plt.hist(raw_features[key], bins=100, color='skyblue', edgecolor='k', alpha=0.7)\n",
        "    mean_val = np.mean(raw_features[key])\n",
        "    plt.axvline(mean_val, color='red', linestyle='dashed', linewidth=2)\n",
        "    plt.title(f'{key} (Raw)\\nMean = {mean_val:.2f}')\n",
        "    plt.xlabel(key)\n",
        "    plt.yscale('log')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Z-score plot\n",
        "    plt.subplot(4, 2, i*2 + 2)\n",
        "    plt.hist(scaled_features[key], bins=100, color='lightgreen', edgecolor='k', alpha=0.7)\n",
        "    plt.axvline(0, color='red', linestyle='dashed', linewidth=2)\n",
        "    plt.title(f'{key} (Z-score Scaled)')\n",
        "    plt.xlabel(f'{key} (z-score)')\n",
        "    plt.yscale('log')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb25c085",
      "metadata": {},
      "source": [
        "## CNN model "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea16af88",
      "metadata": {},
      "source": [
        "![conv2d_scheme](./img/conv2d.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "336666b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiInputCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiInputCNN, self).__init__()\n",
        "\n",
        "        # Branch A: (1, 3, 16)\n",
        "        self.branch_a = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, padding=1),  # → (8, 3, 16)\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),               # → (8, 1, 1) Reduces each feature map to a single value by averaging all its values.\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Branch B: (1, 12, 12)\n",
        "        self.branch_b = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Branch C: (1, 12, 6)\n",
        "        self.branch_c = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Fully connected head\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(8 * 3, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # regression output\n",
        "        )\n",
        "\n",
        "    def forward(self, x_a, x_b, x_c):\n",
        "        out_a = self.branch_a(x_a)  # input: (B, 1, 3, 16)\n",
        "        out_b = self.branch_b(x_b)  # input: (B, 1, 12, 12)\n",
        "        out_c = self.branch_c(x_c)  # input: (B, 1, 12, 6)\n",
        "\n",
        "        x = torch.cat([out_a, out_b, out_c], dim=1)\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67c79bf8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, particle_data, indices=None, normalize=True, stats=None):\n",
        "        self.indices = indices if indices is not None else range(len(particle_data.energy))\n",
        "        self.layer_0 = particle_data.layer_0[self.indices]\n",
        "        self.layer_1 = particle_data.layer_1[self.indices]\n",
        "        self.layer_2 = particle_data.layer_2[self.indices]\n",
        "        \n",
        "        self.energy  = particle_data.energy[self.indices]\n",
        "        self.normalize = normalize\n",
        "        self.stats = stats  # dict with mean/std for each layer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.energy)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        l0 = self.layer_0[idx]\n",
        "        l1 = self.layer_1[idx]\n",
        "        l2 = self.layer_2[idx]\n",
        "        e = self.energy[idx]\n",
        "\n",
        "        if self.normalize and self.stats:\n",
        "            l0 = (l0 - self.stats['l0_mean']) / (self.stats['l0_std'] + 1e-6)\n",
        "            l1 = (l1 - self.stats['l1_mean']) / (self.stats['l1_std'] + 1e-6)\n",
        "            l2 = (l2 - self.stats['l2_mean']) / (self.stats['l2_std'] + 1e-6)\n",
        "            e  = (e  - self.stats['energy_mean']) / (self.stats['energy_std'] + 1e-6)\n",
        "\n",
        "        x_a = torch.tensor(l0, dtype=torch.float32).unsqueeze(0)  # (1, 3, 16)\n",
        "        x_b = torch.tensor(l1, dtype=torch.float32).unsqueeze(0)  # (1, 12, 12)\n",
        "        x_c = torch.tensor(l2, dtype=torch.float32).unsqueeze(0)  # (1, 12, 6)\n",
        "        y   = torch.tensor(e, dtype=torch.float32).squeeze()\n",
        "\n",
        "        return x_a, x_b, x_c, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f767a123",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab30664b",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x_a, x_b, x_c, y in val_loader:\n",
        "        x_a, x_b, x_c, y = x_a.to(device), x_b.to(device), x_c.to(device), y.to(device)\n",
        "        outputs = model(x_a, x_b, x_c)\n",
        "        loss = criterion(outputs, y.unsqueeze(1).float())\n",
        "        total_loss += loss.item() * x_a.size(0)\n",
        "\n",
        "    return total_loss / len(val_loader.dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4c2ffac",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader=None, epochs=10, lr=0.001, checkpoint_path='best_model.pth'):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for x_a, x_b, x_c, y in train_loader:\n",
        "            x_a, x_b, x_c, y = x_a.to(device), x_b.to(device), x_c.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(x_a, x_b, x_c)  # shape: (B, 1)\n",
        "            loss = criterion(outputs, y.unsqueeze(1).float())  # ensure shapes match\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * x_a.size(0)\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_losses.append(epoch_train_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_train_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        if val_loader is not None:\n",
        "            val_loss = evaluate_model(model, val_loader, criterion)\n",
        "            val_losses.append(val_loss)\n",
        "            print(f\"  -  Val Loss: {val_loss:.4f}\\n\")\n",
        "            # Checkpoint: save if improved\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                torch.save(model.state_dict(), checkpoint_path)\n",
        "                print(\"[Saved Best Model]\\n\")\n",
        "        else:\n",
        "            val_losses.append(None)\n",
        "\n",
        "    return train_losses, val_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c623e72",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 4. Datasets\n",
        "train_dataset = ParticleDataset(particle_data, indices=train_idx, stats=stats)\n",
        "test_dataset  = ParticleDataset(particle_data, indices=test_idx,  stats=stats)\n",
        "\n",
        "# 5. DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aae347ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = MultiInputCNN()\n",
        "\n",
        "train_losses, test_losses = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=test_loader,\n",
        "    epochs=2,\n",
        "    lr=0.001\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "402265ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment the following lines to load the best model and evaluate it\n",
        "# model = MultiInputCNN()\n",
        "# model.load_state_dict(torch.load('best_model.pth'))\n",
        "# model.to(device)\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8637df4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.yscale('log')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e68ffadb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def scale_input_data(data, stats):\n",
        "    return (data - stats['mean']) / (stats['std'] + 1e-6)\n",
        "def unscale_output_data(data, stats):\n",
        "    return data * (stats['std'] + 1e-6) + stats['mean']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "520f79fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assume you have numpy arrays for one sample\n",
        "x_a_numpy = test_loader.dataset.layer_0 # shape (3,16)\n",
        "x_b_numpy = test_loader.dataset.layer_1  # shape (12,12)\n",
        "x_c_numpy = test_loader.dataset.layer_2  # shape (12,6)\n",
        "\n",
        "# Scale the input data using the training statistics\n",
        "x_a_numpy = scale_input_data(x_a_numpy, {'mean': stats['l0_mean'], 'std': stats['l0_std']})\n",
        "x_b_numpy = scale_input_data(x_b_numpy, {'mean': stats['l1_mean'], 'std': stats['l1_std']})\n",
        "x_c_numpy = scale_input_data(x_c_numpy, {'mean': stats['l2_mean'], 'std': stats['l2_std']})\n",
        "\n",
        "# Convert to tensors and add batch and channel dims\n",
        "x_a = torch.tensor(x_a_numpy, dtype=torch.float32).unsqueeze(1).to(device)  # (1,1,3,16)\n",
        "x_b = torch.tensor(x_b_numpy, dtype=torch.float32).unsqueeze(1).to(device)  # (1,1,12,12)\n",
        "x_c = torch.tensor(x_c_numpy, dtype=torch.float32).unsqueeze(1).to(device)  # (1,1,12,6)\n",
        "\n",
        "with torch.no_grad():\n",
        "    ypred = model(x_a, x_b, x_c).cpu().numpy()\n",
        "# Unscale the output data using the training statistics\n",
        "ypred = unscale_output_data(ypred, {'mean': stats['energy_mean'], 'std': stats['energy_std']})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42bb0e36",
      "metadata": {},
      "outputs": [],
      "source": [
        "ytest = test_loader.dataset.energy  # Get the first sample's energy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26093f47",
      "metadata": {},
      "outputs": [],
      "source": [
        "ypred = ypred.squeeze()\n",
        "ytest = ytest.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd14082a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(\"MSE: %.7f\" % mean_squared_error(ytest, ypred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "facd84df",
      "metadata": {},
      "source": [
        "The **R² score** is a statistical metric that measures how well a regression model explains the **variance** in the target variable.\n",
        "\n",
        "It is defined as:\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $SS_{\\text{res}}$ = sum of squared residuals (errors)\n",
        "- $ SS_{\\text{tot}} = \\sum (\\bar{y} - y_i)^2 $= total sum of squares\n",
        "\n",
        "---\n",
        "R² tells you the proportion of variance in the target variable that is explained by the model.\n",
        "  - An R² of **1.0** means perfect prediction.\n",
        "  - An R² of **0.0** means the model does no better than predicting the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0b6156e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sklearn\n",
        "\n",
        "par = np.polyfit(ypred.squeeze(), ytest.squeeze(),1)\n",
        "r2score = sklearn.metrics.r2_score(par[0]*ypred.squeeze()+par[1], ytest.squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a30fdad",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.hist2d(par[0]*ypred.squeeze()+par[1], ytest.squeeze(),bins = 100,norm = LogNorm())\n",
        "plt.text(0.05, 0.95, f\"$R^2$ = {r2score:.4f}\", transform=plt.gca().transAxes,\n",
        "         fontsize=12, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.7))\n",
        "plt.plot([0,100],[0,100],'r--')\n",
        "plt.grid()\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.colorbar(label='Counts')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48b0cd0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(ypred - ytest, bins=np.linspace(-10,10,70), alpha=0.7, label='Residuals')\n",
        "plt.axvline(0, color='red', linestyle='--', label='Zero Line')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Residuals Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "375ff830",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming these are unnormalized (original units):\n",
        "# y_true and y_pred: shape (N,)\n",
        "residuals = ypred - ytest\n",
        "#compute relative error\n",
        "RError = residuals / ytest  # relative error\n",
        "\n",
        "# Define energy bins\n",
        "# bins = np.linspace(ytest.min(), ytest.max(), num=10)  # e.g., 10 bins\n",
        "bins = np.linspace(0, 100, num=11)  # e.g., 10 bins\n",
        "bin_indices = np.digitize(ytest, bins)\n",
        "\n",
        "# Create DataFrame for easy grouping\n",
        "df = pd.DataFrame({\n",
        "    'Energy': ytest,\n",
        "    'RError': RError,\n",
        "    'Bin': bin_indices\n",
        "})\n",
        "\n",
        "# Group by bin\n",
        "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
        "residuals_by_bin = [df[df['Bin'] == i]['RError'] for i in range(1, len(bins))]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.boxplot(residuals_by_bin, positions=bin_centers, widths=5, \n",
        "            showfliers=False # to remvoe outliers\n",
        "            \n",
        "            )\n",
        "#print text over each box to show the mean value\n",
        "for i, res in enumerate(residuals_by_bin):\n",
        "    mean_res = res.median()\n",
        "    plt.text(bin_centers[i], mean_res, f'{mean_res *100:.2f}%', ha='center', va='bottom')\n",
        "plt.xlabel('True Energy')\n",
        "plt.ylabel('Residual (Predicted - True)')\n",
        "plt.title('Residuals by Energy Bin')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e6ed1ac",
      "metadata": {},
      "source": [
        "## Time to Excercise\n",
        "Try to train a particle classificator using the 3 dataset above."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a63c0331",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
